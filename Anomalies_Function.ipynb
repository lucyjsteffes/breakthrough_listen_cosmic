{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9a96d1",
   "metadata": {},
   "source": [
    "# A Notebook to begin Filtering RFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6b004",
   "metadata": {},
   "source": [
    "This is the second notebook that I made while working on ARTISTIC. It is still a little bit less focused than some of the other notebooks that come after this one because I did not have as clear of an idea about how I would go about narrowing down 5.7 million hits to a handful of hits to be able to comb through individually. This notebook starts out by going through and taking out all of the hits that were present in all six beams at the same frequency in the same field of view. This is one of the things that I practiced doing in the Practice Hits Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa795b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61147930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5697746, 24)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('../../Preliminary_Data_Files/hits_collation.json')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687216d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22826\n",
      "1721\n"
     ]
    }
   ],
   "source": [
    "######### Looking at the number of hits in each field of view #########\n",
    "file_path = df[\"file_path\"]\n",
    "unique_file_path = np.unique(file_path) #the array of unique fields of view\n",
    "print(len(unique_file_path))\n",
    "print(len(np.unique(df[\"tstart\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Start by making a blank data frame that we can append information to #########\n",
    "df1 = pd.DataFrame({\"file_path\":[],\n",
    "                    \"hit_file_enumeration\":[],\n",
    "                    \"signal_frequency\":[],\n",
    "                    \"signal_index\":[],\n",
    "                    \"signal_driftSteps\":[],\n",
    "                    \"signal_driftRate\":[],\n",
    "                    \"signal_snr\":[],\n",
    "                    \"signal_coarseChannel\":[],\n",
    "                    \"signal_numTimesteps\":[],\n",
    "                    \"signal_power\":[],\n",
    "                    \"signal_incoherentPower\":[],\n",
    "                    \"sourceName\":[],\n",
    "                    \"fch1\":[],\n",
    "                    \"foff\":[],\n",
    "                    \"tstart\":[],\n",
    "                    \"tsamp\":[],\n",
    "                    \"ra\":[],\n",
    "                    \"dec\":[],\n",
    "                    \"telescopeId\":[],\n",
    "                    \"numTimesteps\":[],\n",
    "                    \"numChannels\":[],\n",
    "                    \"coarseChannel\":[],\n",
    "                    \"startChannel\":[],\n",
    "                    \"beam\":[]})\n",
    "\n",
    "######## This for loop should go through and add all of the tables that have the beam anomalies to a new table #########\n",
    "for i in tqdm(range(len(unique_file_path))):\n",
    "#for i in range(1):\n",
    "    fov_subset = df.loc[df['file_path'] == unique_file_path[i]] #select each subset\n",
    "    fov_subset = fov_subset.sort_values(by = [\"signal_frequency\"])\n",
    "    \n",
    "    freq = fov_subset[\"signal_frequency\"] #define just the column for frequencies\n",
    "    power = fov_subset[\"signal_power\"] #define just the column for power\n",
    "    unique_freq = np.unique(freq) #find the array of unique frequencies\n",
    "\n",
    "    #Here we want to find how many hits each frequency had between the 5 coherent beams and the one incoherent beam\n",
    "    hits_per_freq = []\n",
    "    for k in range(len(unique_freq)):\n",
    "        hits_per_freq.append(np.count_nonzero(freq == unique_freq[k]))\n",
    "    #print(hits_per_freq)\n",
    "\n",
    "    #Each frequency should have a maximum of 6 hits if it is widespread RFI\n",
    "    indices = np.where(np.array(hits_per_freq) != 6)\n",
    "    values_less_than_6 = np.array(unique_freq)[indices]\n",
    "    \n",
    "    new_indices = np.concatenate(np.where(np.isin(np.array(freq), values_less_than_6) == True))\n",
    "    \n",
    "    appending_rows = fov_subset.iloc[new_indices]\n",
    "    \n",
    "    df1 = df1.append(appending_rows, ignore_index = True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subset_3.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(df1, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = []\n",
    "#for i in range(len(unique_file_path)):\n",
    "for i in range(100):\n",
    "    fov_subset = df.loc[df['file_path'] == unique_file_path[i]] #select each subset\n",
    "    \n",
    "    freq = fov_subset[\"signal_frequency\"] #define just the column for frequencies\n",
    "    power = fov_subset[\"signal_power\"] #define just the column for power\n",
    "    unique_freq = np.unique(freq) #find the array of unique frequencies\n",
    "\n",
    "    #Here we want to find how many hits each frequency had between the 5 coherent beams and the one incoherent beam\n",
    "    hits_per_freq = []\n",
    "    for k in range(len(unique_freq)):\n",
    "        hits_per_freq.append(np.count_nonzero(freq == unique_freq[k]))\n",
    "\n",
    "    #Each frequency should have a maximum of 6 hits if it is widespread RFI\n",
    "    indices = np.where(np.array(hits_per_freq) < 6)\n",
    "    #print(indices)\n",
    "    values_less_than_6 = np.array(unique_freq)[indices]\n",
    "    #print(values_less_than_6)\n",
    "    #print(np.array(hits_per_freq)[indices])\n",
    "    \n",
    "    new_indices = np.where(np.isin(np.array(fov_subset.sort_values(by = [\"signal_frequency\", \"beam\"])[\"signal_frequency\"]), \n",
    "                      values_less_than_6) == True)\n",
    "    \n",
    "    print(new_indices)\n",
    "    \n",
    "    time = np.array(fov_subset.sort_values(by = [\"signal_frequency\", \"beam\"])[\"tstart\"])[new_indices]\n",
    "    start_time.append(time)\n",
    "    \n",
    "    print(i, ' out of ', len(unique_file_path))\n",
    "start_time  = np.concatenate(start_time)\n",
    "print(len(np.unique(start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ece74",
   "metadata": {},
   "source": [
    "Now open up that .pickle file and make sure that it is everything that we were trying to get\n",
    "\n",
    "Printing out some things just to see the shape of the data we are working with. Does not need to be run each time because nothing gets defined.\n",
    "\n",
    "Now just looking at a sample plot to see what the frequencies of the hits are. Also does not need to be run each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "g = sns.relplot(data=df1, x=df1.iloc[:][\"ra\"], y=df1.iloc[:][\"dec\"], hue=df1.iloc[:][\"signal_frequency\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bcb9a",
   "metadata": {},
   "source": [
    "Define a few variables. Should be run each time because a lot of things reference this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8534a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = np.unique(subset_2[\"Source Name\"]) #The names of the 7732 unique sources (plus the incoherent beam)\n",
    "ra = subset_2[\"Right Ascension\"]\n",
    "dec = subset_2[\"Declination\"]\n",
    "\n",
    "print(len(source_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292693ab",
   "metadata": {},
   "source": [
    "Open the pickle file that has the incorrect information for the number of fields per unique source. Could honestly be removed because that file is not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_fields_1.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(number_of_fovs, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207d82e",
   "metadata": {},
   "source": [
    "Arrange the information in the subset pickle file based on a few characteristics. We will filter based on this information. It could be a good idea to toss this into a pickle file because it tends to take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.DataFrame({'Source': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Source Name\"]), \n",
    "        'File Path': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"File Path\"]), \n",
    "        'Right Ascension': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Right Ascension\"]), \n",
    "       'Declination': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Declination\"])})\n",
    "\n",
    "display(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fe667",
   "metadata": {},
   "source": [
    "This sets up the indexing and figuring out how many unique fields of view there are per source. It takes a long time to run and because the information is already calculated and being stored in a pickle file, should not be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = np.unique(subset_1[\"Source Name\"])\n",
    "source = subset_1[\"Source Name\"]\n",
    "\n",
    "hits_per_source = []\n",
    "for i in range(len(source_name)):\n",
    "    hits_per_source.append(np.count_nonzero(source == source_name[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be744091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(source_name), len(source))\n",
    "\n",
    "print(hits_per_source[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f235d6a",
   "metadata": {},
   "source": [
    "We might need the indices again and this one doesn't take long to run, so it should be run every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0]\n",
    "for i in range(len(hits_per_source)-1):\n",
    "    indices.append(indices[i] + hits_per_source[i])\n",
    "    \n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a959a1",
   "metadata": {},
   "source": [
    "Do not run this again. All of the information has already been stored in a pickle file, so you just need to open the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff73c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_info = pd.DataFrame({'Source': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Source Name\"])[indices], \n",
    "        'Number of Hits': np.array(hits_per_source), \n",
    "        'Right Ascension': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Right Ascension\"])[indices], \n",
    "       'Declination': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Declination\"])[indices]})\n",
    "\n",
    "display(hits_info)\n",
    "\n",
    "#with open('hits_per_source.pkl', 'wb') as f:  # open a text file\n",
    "    #pickle.dump(hits_info, f) # serialize the list\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223794d6",
   "metadata": {},
   "source": [
    "Be sure to open this each time. The naming conventions are the the same for below so there should be no issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6148c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hits_per_source.pkl', 'rb') as f:\n",
    "    hits_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d734f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.relplot(data=hits_info[0:-1], x=hits_info[\"Right Ascension\"][0:-1], y=hits_info[\"Declination\"][0:-1], \n",
    "            hue=hits_info[\"Number of Hits\"][0:-1], size = hits_info[\"Number of Hits\"][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb806257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "\n",
    "def countFreq(arr, n):\n",
    "    \n",
    "    # Mark all array elements as not visited\n",
    "    visited = [False for i in range(n)]\n",
    "    fp_list = []\n",
    "    # Traverse through array elements and count frequencies\n",
    "    for i in range(n):\n",
    "         \n",
    "        # Skip this element if already processed\n",
    "        if (visited[i] == True):\n",
    "            continue\n",
    " \n",
    "        # Count frequency\n",
    "        count = 1\n",
    "        for j in range(i + 1, n, 1):\n",
    "            if (arr[i] == arr[j]):\n",
    "                visited[j] = True\n",
    "                count += 1\n",
    "         \n",
    "        #print(count)\n",
    "        fp_list.append(count)\n",
    "    return len(fp_list)\n",
    "        \n",
    "#Driver Code\n",
    "arr = np.array(info[\"File Path\"][693038:-1])\n",
    "n = len(arr)\n",
    "countFreq(arr, n)\n",
    "#countFreq(np.array(info[\"File Path\"]), len(np.array(info[\"File Path\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6514666",
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs_per_source = []\n",
    "for i in range(len(indices)-1):\n",
    "    arr = np.array(info[\"File Path\"][indices[i]:indices[i+1]])\n",
    "    n = len(arr)\n",
    "    fovs_per_source.append(countFreq(arr, n))\n",
    "#print(len(fovs_per_source))\n",
    "fovs_per_source.append(18194)\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    sleep(3)\n",
    "#print(indices[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs_info = pd.DataFrame({'Source': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Source Name\"])[indices], \n",
    "        'Number of FOVs': np.array(fovs_per_source), \n",
    "        'Right Ascension': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Right Ascension\"])[indices], \n",
    "       'Declination': np.array(subset_1.sort_values(by = [\"Source Name\", \"File Path\"])[\"Declination\"])[indices]})\n",
    "\n",
    "display(fovs_info)\n",
    "\n",
    "with open('fovs_per_source.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(fovs_info, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fovs_info[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a67ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fovs_per_source.pkl', 'rb') as f:\n",
    "    fovs_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.relplot(data=fovs_info[0:-1], x=fovs_info[\"Right Ascension\"][0:-1], y=fovs_info[\"Declination\"][0:-1], \n",
    "            hue=fovs_info[\"Number of FOVs\"][0:-1], size = fovs_info[\"Number of FOVs\"][0:-1], palette = \"ch:s=.25,rot=-.25\")\n",
    "\n",
    "#plt.savefig('FOVs_per_Source.png', bbox_inches='tight', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(fovs_info[\"Number of FOVs\"][0:-1]))\n",
    "print(np.min(fovs_info[\"Number of FOVs\"][0:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf16b6f",
   "metadata": {},
   "source": [
    "Ok now we need to start looking through each of the fields of view to see where there might be similar frequencies and characteristics. So let's start by trying to just pull all of the data for one source with a lot of fields of view. By looking at the table above, I'm going to start by just choosing a source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196aa073",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_1 = subset_1.loc[subset_1[\"Source Name\"] == \"10132416262554752\"]\n",
    "\n",
    "display(source_1[0:20])\n",
    "\n",
    "print(np.array(source_1[\"Signal to Noise\"]))\n",
    "\n",
    "print(len(subset_1[\"Signal Index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68166b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(source_1[\"Frequency\"])))\n",
    "print(len(source_1[\"Frequency\"]))\n",
    "#print(np.array(source_1[\"Frequency\"]))\n",
    "\n",
    "def countFreq(arr, n):\n",
    "    \n",
    "    # Mark all array elements as not visited\n",
    "    visited = [False for i in range(n)]\n",
    "    # Traverse through array elements and count frequencies\n",
    "    for i in range(n):\n",
    "         \n",
    "        # Skip this element if already processed\n",
    "        if (visited[i] == True):\n",
    "            continue\n",
    " \n",
    "        # Count frequency\n",
    "        count = 1\n",
    "        for j in range(i + 1, n, 1):\n",
    "            if (arr[i] == arr[j]):\n",
    "                visited[j] = True\n",
    "                count += 1\n",
    "         \n",
    "        print(arr[i], count)\n",
    "        #fp_list.append(count)\n",
    "        \n",
    "#Driver Code\n",
    "arr = np.array(source_1[\"Frequency\"])\n",
    "n = len(arr)\n",
    "countFreq(arr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc76aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.relplot(data=source_1, x=source_1[\"Frequency\"], y=source_1[\"Drift Rate\"], \n",
    "            hue=source_1[\"Signal to Noise\"], size = source_1[\"Signal to Noise\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
