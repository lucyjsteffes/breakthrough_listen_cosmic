{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce62f659",
   "metadata": {},
   "source": [
    "# Splitting by the Number of Beams per Hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dbbdf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from astropy.time import Time\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9db3ce",
   "metadata": {},
   "source": [
    "Ok so to start, I want to start filtering by the signal to noise ratio. In doing this, we make an assumption about the potential power of alien civilizations, so we start with a very high signal to noise ratio to cap at (100).\n",
    "\n",
    "Now start by opening the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dedead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805064, 24)\n"
     ]
    }
   ],
   "source": [
    "with open('../../Pickle_Files/subset_3.pkl', 'rb') as f:\n",
    "\n",
    "    data = pickle.load(f) # deserialize using load()\n",
    "    \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92529bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_filtered = data[data[\"signal_snr\"] < 100]\n",
    "\n",
    "with open('snr_filtered.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(snr_filtered, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978dc68",
   "metadata": {},
   "source": [
    "Now that we have filtered by the signal to noise ratio, we want to go through and only keep the hits which have a corresponding hit in both the incoherent and coherent beams. So how do I do this? I think it would make sense to set up a similar indexing to what I had in the preliminary filtering. From there, it could make sense to get rid of all of the single beam hits, which would either be a hit in only the incoherent beam, or a hit in one of the coherent beams with nothing in the incoherent beam.\n",
    "\n",
    "Save that all as another pickle file so that I don't have to keep running these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d278e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772551, 24)\n"
     ]
    }
   ],
   "source": [
    "with open('snr_filtered.pkl', 'rb') as f:\n",
    "\n",
    "    snr_data = pickle.load(f) # deserialize using load()\n",
    "    \n",
    "print(snr_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb928016",
   "metadata": {},
   "source": [
    "Now Jared has sent some of the preliminary kurtosis information. So I want to remove any of the channels that have a lot of RFI before continuing.\n",
    "\n",
    "Start by opening the csv file with the frequency binning information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '../../Preliminary_Data_Files/prelim_kurt_bins.csv'\n",
    "#csv.reader(source)\n",
    "\n",
    "file = open(source)\n",
    "\n",
    "with open(source, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    row1 = next(reader)\n",
    "\n",
    "data = pd.read_csv(source)\n",
    "\n",
    "lower_freq_bin = [float(x) for x in data['rfi_freq_bin_bots'] if str(x) != 'nan']\n",
    "upper_freq_bin = [float(x) for x in data['rfi_freq_bin_tops'] if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9720b",
   "metadata": {},
   "source": [
    "Now find all of the frequencies which are in between the lower and upper edges of the frequency bins. Index them and then remove these indices from the original list of indices so that we are left with just the indices which we would like to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf72cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_frequency = np.array(snr_data['signal_frequency']).tolist()\n",
    "\n",
    "source_name = np.array(snr_data['sourceName'])\n",
    "unique_source_name = np.unique(source_name)\n",
    "\n",
    "discarded_frequencies = []\n",
    "for i in tqdm(range(len(source_name))):\n",
    "    for j in range(len(lower_freq_bin)):\n",
    "        if signal_frequency[i] > lower_freq_bin[j] and signal_frequency[i] < upper_freq_bin[j]:\n",
    "            discarded_frequencies.append(i)\n",
    "\n",
    "kept_frequencies = list(set(range(len(signal_frequency))) - set(discarded_frequencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9416f",
   "metadata": {},
   "source": [
    "Now run through and keep just those rows in the table which correspond to the indices which we decided above we would like to keep. We still need to define the data frame that we are appending too, but the rest is pretty easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"file_path\":[],\n",
    "                    \"hit_file_enumeration\":[],\n",
    "                    \"signal_frequency\":[],\n",
    "                    \"signal_index\":[],\n",
    "                    \"signal_driftSteps\":[],\n",
    "                    \"signal_driftRate\":[],\n",
    "                    \"signal_snr\":[],\n",
    "                    \"signal_coarseChannel\":[],\n",
    "                    \"signal_numTimesteps\":[],\n",
    "                    \"signal_power\":[],\n",
    "                    \"signal_incoherentPower\":[],\n",
    "                    \"sourceName\":[],\n",
    "                    \"fch1\":[],\n",
    "                    \"foff\":[],\n",
    "                    \"tstart\":[],\n",
    "                    \"tsamp\":[],\n",
    "                    \"ra\":[],\n",
    "                    \"dec\":[],\n",
    "                    \"telescopeId\":[],\n",
    "                    \"numTimesteps\":[],\n",
    "                    \"numChannels\":[],\n",
    "                    \"coarseChannel\":[],\n",
    "                    \"startChannel\":[],\n",
    "                    \"beam\":[]})\n",
    "\n",
    "appending_rows = snr_data.iloc[kept_frequencies]\n",
    "    \n",
    "df1 = df1.append(appending_rows, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0af03",
   "metadata": {},
   "source": [
    "And now save that data frame to a pickle file so that we can open it and work with that information whenever we want to and need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('snr_kurtosis.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(df1, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('snr_kurtosis.pkl', 'rb') as f:\n",
    "\n",
    "    snr_data_kurtosis = pickle.load(f) # deserialize using load()\n",
    "    \n",
    "print(snr_data_kurtosis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991df6e",
   "metadata": {},
   "source": [
    "Comparing where the data in the data frames before and after the kurtosis cut were distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Looking at the number of hits in each field of view #########\n",
    "file_path = snr_data[\"file_path\"]\n",
    "unique_file_path = np.unique(file_path) #the array of unique fields of view\n",
    "print(len(unique_file_path))\n",
    "print(len(np.unique(snr_data[\"tstart\"])))\n",
    "\n",
    "file_path_kurtosis = snr_data_kurtosis[\"file_path\"]\n",
    "unique_file_path_kurtosis = np.unique(file_path) #the array of unique fields of view\n",
    "print(len(unique_file_path_kurtosis))\n",
    "print(len(np.unique(snr_data_kurtosis[\"tstart\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ab62e",
   "metadata": {},
   "source": [
    "Now I would like to split the file up into two smaller .pickle files. This split is based on the number of beams that a hit is present in. Because I want to compare the powers between the incoherent beam and the coherent beams, I want to take out all of the hits which are only present in one beam.\n",
    "\n",
    "There are two reasons why a hit might be present in only one beam. If the signal is incredibly weak, then it might get averaged out in the incoherent beam, but would still be present in a coherent beam. However, if the signal is not localized to one of the 5 closest stars within a given field of view, then it might only be present in the incoherent beam. Thsi is the case with something like Voyager or if there were aliens on a rogue planet or on a space ship flying around in the ISM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded74fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Start by making a blank data frame that we can append information to #########\n",
    "df2 = pd.DataFrame({\"file_path\":[],\n",
    "                    \"hit_file_enumeration\":[],\n",
    "                    \"signal_frequency\":[],\n",
    "                    \"signal_index\":[],\n",
    "                    \"signal_driftSteps\":[],\n",
    "                    \"signal_driftRate\":[],\n",
    "                    \"signal_snr\":[],\n",
    "                    \"signal_coarseChannel\":[],\n",
    "                    \"signal_numTimesteps\":[],\n",
    "                    \"signal_power\":[],\n",
    "                    \"signal_incoherentPower\":[],\n",
    "                    \"sourceName\":[],\n",
    "                    \"fch1\":[],\n",
    "                    \"foff\":[],\n",
    "                    \"tstart\":[],\n",
    "                    \"tsamp\":[],\n",
    "                    \"ra\":[],\n",
    "                    \"dec\":[],\n",
    "                    \"telescopeId\":[],\n",
    "                    \"numTimesteps\":[],\n",
    "                    \"numChannels\":[],\n",
    "                    \"coarseChannel\":[],\n",
    "                    \"startChannel\":[],\n",
    "                    \"beam\":[]})\n",
    "\n",
    "######## This for loop should go through and add all of the tables that have the beam anomalies to a new table #########\n",
    "for i in tqdm(range(len(unique_file_path_kurtosis))):\n",
    "    fov_subset = snr_data_kurtosis.loc[snr_data_kurtosis['file_path'] == \n",
    "                                       unique_file_path_kurtosis[i]].sort_values(by = [\"signal_frequency\"]) \n",
    "    #select each subset\n",
    "    \n",
    "    freq = np.array(fov_subset[\"signal_frequency\"]) #define just the column for frequencies\n",
    "    unique_freq = np.unique(freq) #find the array of unique frequencies\n",
    "    \n",
    "    #Here we want to find how many hits each frequency had between the 5 coherent beams and the one incoherent beam\n",
    "    hits_per_freq = []\n",
    "    for k in range(len(unique_freq)):\n",
    "        hits_per_freq.append(np.count_nonzero(np.array(freq) == unique_freq[k]))\n",
    "   \n",
    "    #Each frequency should have a maximum of 6 hits if it is widespread RFI\n",
    "    #indices = np.where(np.array(hits_per_freq) == 1)\n",
    "    indices = np.where(np.array(hits_per_freq) != 1)\n",
    "    values_greater_than_1 = np.array(unique_freq)[indices]\n",
    "    \n",
    "    new_indices = np.concatenate(np.where(np.isin(np.array(fov_subset.sort_values(by = [\"signal_frequency\", \"beam\"])\n",
    "                                                           [\"signal_frequency\"]), values_greater_than_1) == True))\n",
    "    \n",
    "    appending_rows = fov_subset.iloc[new_indices]\n",
    "    \n",
    "    df2 = df2.append(appending_rows, ignore_index = True)\n",
    "    \n",
    "with open('greater_than_1_kurtosis.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(df2, f) # serialize the list\n",
    "f.close()\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80d782",
   "metadata": {},
   "source": [
    "Ok now index everything again, but this time, try to figure out how to make it into subarrays or a list of smaller arrays. Then see if there is a way to only keep the sets of arrays that only contain a 5 (incoherent beam). But we'll start this with just the hits that are present in 4 and 5 beams. Because I have already removed the hits that are present in all 6 beams, these are the most. It is most likely low intensity RFI if it is present in 4-5 beams but no incoherent beam, but this will be checked by examining the SNR and the total power.\n",
    "\n",
    "The reason I am not looking at the hits that are present in 2-3 beams first is because there are more reasons as to why these might not have a corresponding incoherent beam. If the two unique sources are very close to each other to the point where they would not be fully resolved, then we would see the same hit occurring in multiple beams. So once we have these split up, I'll look at how far apart the beams are from each other to test this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greater_than_1_kurtosis.pkl', 'rb') as f:\n",
    "\n",
    "    greater_than_1_kurtosis = pickle.load(f) # deserialize using load()\n",
    "    print(greater_than_1_kurtosis.shape)\n",
    "    \n",
    "with open('1_hit_kurtosis.pkl', 'rb') as f:\n",
    "\n",
    "    data_1_kurtosis = pickle.load(f) # deserialize using load()\n",
    "    print(data_1_kurtosis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Looking at the number of hits in each field of view #########\n",
    "file_path = greater_than_1_kurtosis[\"file_path\"]\n",
    "unique_file_path = np.unique(file_path) #the array of unique fields of view\n",
    "print(greater_than_1_data.shape)\n",
    "print(len(unique_file_path))\n",
    "print(len(np.unique(greater_than_1_data[\"tstart\"])))\n",
    "\n",
    "file_path_1 = data_1_kurtosis[\"file_path\"]\n",
    "unique_file_path_1 = np.unique(file_path_1) #the array of unique fields of view\n",
    "print(data_1_kurtosis.shape)\n",
    "print(len(unique_file_path_1))\n",
    "print(len(np.unique(data_1[\"tstart\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919aa84",
   "metadata": {},
   "source": [
    "This next section is looking to map the observations we are currently left with. I use seaborn to quickly show the frequencies and the number of hits per source or per field of view in each of these plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = np.unique(greater_than_1_kurtosis[\"sourceName\"])\n",
    "source = greater_than_1_kurtosis[\"sourceName\"]\n",
    "\n",
    "hits_per_source = []\n",
    "for i in tqdm(range(len(source_name))):\n",
    "    hits_per_source.append(np.count_nonzero(source == source_name[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cab7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0]\n",
    "for i in range(len(hits_per_source)-1):\n",
    "    indices.append(indices[i] + hits_per_source[i])\n",
    "    \n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_info = pd.DataFrame({'Source': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"sourceName\"])[indices], \n",
    "        'Number of Hits': np.array(hits_per_source), \n",
    "        'Right Ascension': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"ra\"])[indices], \n",
    "       'Declination': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"dec\"])[indices]})\n",
    "\n",
    "display(hits_info)\n",
    "\n",
    "with open('hits_per_source_2.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(hits_info, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0394f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hits_per_source_2.pkl', 'rb') as f:\n",
    "    hits_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.relplot(data=hits_info[0:-1], x=hits_info[\"Right Ascension\"][0:-1], y=hits_info[\"Declination\"][0:-1], \n",
    "            hue=hits_info[\"Number of Hits\"][0:-1], size = hits_info[\"Number of Hits\"][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.DataFrame({'Source': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"sourceName\"]), \n",
    "        'File Path': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"file_path\"]), \n",
    "        'Right Ascension': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"ra\"]), \n",
    "       'Declination': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"dec\"])})\n",
    "\n",
    "display(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22aad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFreq(arr, n):\n",
    "    \n",
    "    # Mark all array elements as not visited\n",
    "    visited = [False for i in range(n)]\n",
    "    fp_list = []\n",
    "    # Traverse through array elements and count frequencies\n",
    "    for i in range(n):\n",
    "         \n",
    "        # Skip this element if already processed\n",
    "        if (visited[i] == True):\n",
    "            continue\n",
    " \n",
    "        # Count frequency\n",
    "        count = 1\n",
    "        for j in range(i + 1, n, 1):\n",
    "            if (arr[i] == arr[j]):\n",
    "                visited[j] = True\n",
    "                count += 1\n",
    "         \n",
    "        #print(count)\n",
    "        fp_list.append(count)\n",
    "    return len(fp_list)\n",
    "        \n",
    "#Driver Code\n",
    "arr = np.array(info[\"File Path\"][693038:-1])\n",
    "n = len(arr)\n",
    "countFreq(arr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ff0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs_per_source = []\n",
    "for i in range(len(indices)-1):\n",
    "    arr = np.array(info[\"File Path\"][indices[i]:indices[i+1]])\n",
    "    n = len(arr)\n",
    "    fovs_per_source.append(countFreq(arr, n))\n",
    "#print(len(fovs_per_source))\n",
    "fovs_per_source.append(18194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32112a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs_info = pd.DataFrame({'Source': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"sourceName\"])[indices], \n",
    "        'Number of FOVs': np.array(fovs_per_source), \n",
    "        'Right Ascension': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"ra\"])[indices], \n",
    "       'Declination': np.array(greater_than_1_kurtosis.sort_values(by = [\"sourceName\", \"file_path\"])[\"dec\"])[indices]})\n",
    "\n",
    "display(fovs_info)\n",
    "\n",
    "with open('fovs_per_source_2.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(fovs_info, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fovs_per_source_2.pkl', 'rb') as f:\n",
    "    fovs_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f12112",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.relplot(data=fovs_info[0:-1], x=fovs_info[\"Right Ascension\"][0:-1], y=fovs_info[\"Declination\"][0:-1], \n",
    "            hue=fovs_info[\"Number of FOVs\"][0:-1], size = fovs_info[\"Number of FOVs\"][0:-1], palette = \"ch:s=.25,rot=-.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539726bb",
   "metadata": {},
   "source": [
    "Here I am trying to look at which beams are in each of the hits which have multiple beams. That doesn't make much sense. So... Because each possibly interesting hit should have at least one hit in the coherent beam and the same hit in the incoherent beam, we want to make sure that we are only keeping those hits which present themselves in both the incoherent beam and a coherent beam. So here we are going to put each of the sets of hits into smaller array that contain the beam numbers for the corresponding things idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4153e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greater_than_1_kurtosis.pkl', 'rb') as f:\n",
    "    greater_than_1_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eff30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = greater_than_1_data[\"file_path\"]\n",
    "unique_file_path = np.unique(file_path) #the array of unique fields of view\n",
    "\n",
    "print(len(unique_file_path), len(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "array_indexing = []\n",
    "for i in tqdm(range(len(unique_file_path))):\n",
    "    data_fov_subset = greater_than_1_data.loc[greater_than_1_data['file_path'] == unique_file_path[i]] #select each subset    \n",
    "    data_fov_subset_sort = data_fov_subset.sort_values(by = [\"signal_frequency\"], ascending = True)\n",
    "    \n",
    "    freq = np.array(data_fov_subset[\"signal_frequency\"]) #define just the column for frequencies\n",
    "    #print(len(freq))\n",
    "    beam = np.array(data_fov_subset[\"beam\"])\n",
    "    freq_list = freq.tolist()\n",
    "    unique_freq = np.unique(freq) #find the array of unique frequencies\n",
    "    \n",
    "    #Here we want to find how many hits each frequency had between the 5 coherent beams and the one incoherent beam\n",
    "    hits_per_freq = []\n",
    "    for j in range(len(unique_freq)):\n",
    "        hits_per_freq.append(np.count_nonzero(freq == unique_freq[j]))\n",
    "        \n",
    "        this_freq = unique_freq[j]\n",
    "    \n",
    "        frequency_indices = []\n",
    "        index = []\n",
    "        for k in range(len(freq)):\n",
    "            if freq[k] == unique_freq[j]:\n",
    "                frequency_indices.append(beam[k])\n",
    "                index.append(i)\n",
    "\n",
    "        #print(frequency_indices)\n",
    "        array.append(frequency_indices)\n",
    "        array_indexing.append(index)\n",
    "print(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_length = []\n",
    "for i in range(len(array)):\n",
    "    array_length.append(len(array[i]))\n",
    "\n",
    "weirdos = []\n",
    "for i in range(len(array_length)):\n",
    "    if array_length[i] == 5:\n",
    "        if 5.0 not in array[i]:\n",
    "            #print(array[i])\n",
    "            weirdos.append(i)\n",
    "#print(weirdos) #This tells the indexing of which of the smaller arrays have the strange behavior\n",
    "#I am characterizing the weirdos as being the smaller arrays which I would expect to have an incoherent beam, but for whatever \n",
    "#reason do not\n",
    "print(len(weirdos))\n",
    "\n",
    "#Now these next two arrays are looking at which fields of view the \"weirdos\" are present in (numbers) and how many weirdos are\n",
    "#in each field of view (counting)\n",
    "numbers = []\n",
    "for i in range(len(weirdos)):\n",
    "    numbers.append(array_indexing[weirdos[i]][0])\n",
    "\n",
    "counting = []\n",
    "for i in range(len(np.unique(numbers))):\n",
    "    counting.append(np.count_nonzero(numbers == np.unique(numbers)[i]))\n",
    "#print(counting)\n",
    "\n",
    "#print(np.max(counting))\n",
    "#print(np.count_nonzero(np.array(counting) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "five = []\n",
    "for i in range(len(array_length)):\n",
    "    if array_length[i] == 2:\n",
    "        five.append(i)\n",
    "print(len(five))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a44977",
   "metadata": {},
   "source": [
    "I think it'll be easier to not make a mistake if I further break this up into subsets based on the number of beams per hit. And then from there, I should be able to more easily flag those which are lacking an incoherent beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Start by making a blank data frame that we can append information to #########\n",
    "df2 = pd.DataFrame({\"file_path\":[],\n",
    "                    \"hit_file_enumeration\":[],\n",
    "                    \"signal_frequency\":[],\n",
    "                    \"signal_index\":[],\n",
    "                    \"signal_driftSteps\":[],\n",
    "                    \"signal_driftRate\":[],\n",
    "                    \"signal_snr\":[],\n",
    "                    \"signal_coarseChannel\":[],\n",
    "                    \"signal_numTimesteps\":[],\n",
    "                    \"signal_power\":[],\n",
    "                    \"signal_incoherentPower\":[],\n",
    "                    \"sourceName\":[],\n",
    "                    \"fch1\":[],\n",
    "                    \"foff\":[],\n",
    "                    \"tstart\":[],\n",
    "                    \"tsamp\":[],\n",
    "                    \"ra\":[],\n",
    "                    \"dec\":[],\n",
    "                    \"telescopeId\":[],\n",
    "                    \"numTimesteps\":[],\n",
    "                    \"numChannels\":[],\n",
    "                    \"coarseChannel\":[],\n",
    "                    \"startChannel\":[],\n",
    "                    \"beam\":[]})\n",
    "\n",
    "######## This for loop should go through and add all of the tables that have the beam anomalies to a new table #########\n",
    "for i in tqdm(range(len(unique_file_path))):\n",
    "    fov_subset = greater_than_1_data.loc[greater_than_1_data['file_path'] == \n",
    "                                       unique_file_path[i]].sort_values(by = [\"signal_frequency\"]) \n",
    "    #select each subset\n",
    "    \n",
    "    freq = np.array(fov_subset[\"signal_frequency\"]) #define just the column for frequencies\n",
    "    unique_freq = np.unique(freq) #find the array of unique frequencies\n",
    "    \n",
    "    #Here we want to find how many hits each frequency had between the 5 coherent beams and the one incoherent beam\n",
    "    hits_per_freq = []\n",
    "    for k in range(len(unique_freq)):\n",
    "        hits_per_freq.append(np.count_nonzero(np.array(freq) == unique_freq[k]))\n",
    "   \n",
    "    #Change this value here to set it up for different numbers of beams that a hit is present in here\n",
    "    indices = np.where(np.array(hits_per_freq) == 2) \n",
    "\n",
    "    values_greater_than_1 = np.array(unique_freq)[indices]\n",
    "    \n",
    "    new_indices = np.concatenate(np.where(np.isin(np.array(fov_subset.sort_values(by = [\"signal_frequency\", \"beam\"])\n",
    "                                                           [\"signal_frequency\"]), values_greater_than_1) == True))\n",
    "    \n",
    "    appending_rows = fov_subset.iloc[new_indices]\n",
    "    \n",
    "    df2 = df2.append(appending_rows, ignore_index = True)\n",
    "    \n",
    "with open('2_beams_per_hit.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(df2, f) # serialize the list\n",
    "f.close()\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf039e",
   "metadata": {},
   "source": [
    "Now let's open up the .pickle file that has all of the hits that were present in five beams and run the same code as above to figure out which of them are lacking an incoherent beam. Above I called these the weirdos. It should then be easier to index these and then filter all of them out.\n",
    "\n",
    "My plan right now is to clump them together based on the hit. This would be based on the field of view, then based on the frequency. Then within each of those clumps, look at the array of the beam numbers and if beam 5 (the incoherent beam) is present, then the entire clump gets appended to the data frame. But I want to only do this for the hits that are present in 4 or 5 beams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2_beams_per_hit.pkl', 'rb') as f:\n",
    "    hit_2_data = pickle.load(f)\n",
    "print(hit_2_data.shape)\n",
    "file_path_2 = hit_2_data[\"file_path\"]\n",
    "unique_file_path_2 = np.unique(file_path_2) #the array of unique fields of view\n",
    "    \n",
    "with open('3_beams_per_hit.pkl', 'rb') as f:\n",
    "    hit_3_data = pickle.load(f)\n",
    "print(hit_3_data.shape)\n",
    "file_path_3 = hit_3_data[\"file_path\"]\n",
    "unique_file_path_3 = np.unique(file_path_3) #the array of unique fields of view\n",
    "\n",
    "with open('4_beams_per_hit.pkl', 'rb') as f:\n",
    "    hit_4_data = pickle.load(f)\n",
    "print(hit_4_data.shape)\n",
    "file_path_4 = hit_4_data[\"file_path\"]\n",
    "unique_file_path_4 = np.unique(file_path_4) #the array of unique fields of view\n",
    "\n",
    "with open('5_beams_per_hit.pkl', 'rb') as f:\n",
    "    hit_5_data = pickle.load(f)\n",
    "print(hit_5_data.shape)\n",
    "file_path_5 = hit_5_data[\"file_path\"]\n",
    "unique_file_path_5 = np.unique(file_path_5) #the array of unique fields of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb58f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\"file_path\":[],\n",
    "                    \"hit_file_enumeration\":[],\n",
    "                    \"signal_frequency\":[],\n",
    "                    \"signal_index\":[],\n",
    "                    \"signal_driftSteps\":[],\n",
    "                    \"signal_driftRate\":[],\n",
    "                    \"signal_snr\":[],\n",
    "                    \"signal_coarseChannel\":[],\n",
    "                    \"signal_numTimesteps\":[],\n",
    "                    \"signal_power\":[],\n",
    "                    \"signal_incoherentPower\":[],\n",
    "                    \"sourceName\":[],\n",
    "                    \"fch1\":[],\n",
    "                    \"foff\":[],\n",
    "                    \"tstart\":[],\n",
    "                    \"tsamp\":[],\n",
    "                    \"ra\":[],\n",
    "                    \"dec\":[],\n",
    "                    \"telescopeId\":[],\n",
    "                    \"numTimesteps\":[],\n",
    "                    \"numChannels\":[],\n",
    "                    \"coarseChannel\":[],\n",
    "                    \"startChannel\":[],\n",
    "                    \"beam\":[]})\n",
    "\n",
    "for i in tqdm(range(int(len(file_path_2)/2))):\n",
    "#for i in range(4):\n",
    "    data_fov_subset = hit_5_data.loc[i*2:(i*2)+1] #select each subset    \n",
    "    #It is already organized by frequency so we don't need to sort by frequency\n",
    "\n",
    "    #display(data_fov_subset)\n",
    "    freq = np.array(data_fov_subset[\"signal_frequency\"]) #define just the column for frequencies\n",
    "    beam = np.array(data_fov_subset[\"beam\"])\n",
    "    #print(len(beam))\n",
    "    freq_list = freq.tolist()\n",
    "    unique_freq = np.unique(freq) #find the array of unique frequencies\n",
    "    \n",
    "    if 5.0 not in beam:\n",
    "        #print('yay')\n",
    "        appending_rows = data_fov_subset\n",
    "        df2 = df2.append(appending_rows, ignore_index = True)\n",
    "        #print(appending_rows.shape)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "with open('2_beams_coherent.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(df2, f) # serialize the list\n",
    "f.close()\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4_beams.pkl', 'rb') as f:\n",
    "    hit_4 = pickle.load(f)\n",
    "print(hit_4.shape)\n",
    "file_path_4 = hit_4[\"file_path\"]\n",
    "unique_file_path_4 = np.unique(file_path_4) #the array of unique fields of view\n",
    "\n",
    "with open('5_beams.pkl', 'rb') as f:\n",
    "    hit_5 = pickle.load(f)\n",
    "print(hit_5.shape)\n",
    "file_path_5 = hit_5[\"file_path\"]\n",
    "unique_file_path_5 = np.unique(file_path_5) #the array of unique fields of view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095dec3",
   "metadata": {},
   "source": [
    "Now divide the incoherent power by each of the coherent powers of the coherent beams. This should be close to 13 so see if we can print all of these values out. Between all of this, there should be a good way of starting to see some candidates. We can then combine this with the information from the drift rates to get an even better idea of what is going on here. This will be done in other Jupyter Notebooks, which can all be found in this same GitHub Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbfbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31a269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
